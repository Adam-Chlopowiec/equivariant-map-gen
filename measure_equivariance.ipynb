{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Sequence\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import escnn\n",
    "import timm\n",
    "import tqdm\n",
    "from torchvision.transforms import functional as F\n",
    "from escnn import gspaces\n",
    "from PIL import Image\n",
    "from omegaconf import DictConfig\n",
    "\n",
    "import torch\n",
    "from torchvision.transforms import Resize, ToTensor\n",
    "np.set_printoptions(linewidth=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_single_image(config, x: torch.Tensor, N: int = 4, k: int = 5):\n",
    "    x = Image.fromarray(x.cpu().numpy().transpose(1, 2, 0), mode='RGB')\n",
    "\n",
    "    # to reduce interpolation artifacts (e.g. when testing the model on rotated images),\n",
    "    # we upsample an image by a factor of 3, rotate it and finally downsample it again\n",
    "    resize = Resize(224) # to upsample\n",
    "    totensor = ToTensor()\n",
    "    x = resize(x)\n",
    "\n",
    "    # evaluate the model on N rotated versions of the input image x\n",
    "    print()\n",
    "    print('##########################################################################################')\n",
    "    header = 'angle  |  ' + '  '.join([\"{:5d}\".format(d) for d in range(10)])\n",
    "    print(header)\n",
    "    results = np.zeros(shape=(k * N, 10))\n",
    "    with torch.no_grad():\n",
    "        for i in range(1, k+1):\n",
    "            model = hydra.utils.instantiate(config.model) # Hydra\n",
    "            model.eval()\n",
    "            for r in range(N):\n",
    "                x_transformed = totensor(x.rotate(r*360./N, Image.BILINEAR)).reshape(3, 224, 224).unsqueeze(0)\n",
    "\n",
    "                y = model(x_transformed)\n",
    "                y = y.numpy().squeeze()\n",
    "                results[i * r, ðŸ™‚ = y\n",
    "                \n",
    "                angle = r * 360. / N\n",
    "                if i == 1:\n",
    "                    print(\"{:6.1f} : {}\".format(angle, y))\n",
    "    print(f\"mean: {results.mean(axis=0)}\")\n",
    "    print(f\"std: {results.std(axis=0)}\")\n",
    "    print('##########################################################################################')\n",
    "    print()\n",
    "\n",
    "\n",
    "def compare_two_models_relative_errors(x, model, eq_model, W=225):\n",
    "    # r2_act = gspaces.rot2dOnR2(-1, maximum_frequency=32)\n",
    "    # r2_act = gspaces.flipRot2dOnR2(-1, maximum_frequency=32)\n",
    "    N = 4\n",
    "    r2_act = gspaces.flipRot2dOnR2(N)\n",
    "    G = r2_act.fibergroup\n",
    "    x = Resize(W)(x)\n",
    "    # create the mask for the input\n",
    "    input_center_mask = torch.ones_like(x)\n",
    "\n",
    "    # mask the input image\n",
    "    x = x * input_center_mask\n",
    "    x = eq_model.in_type(x)\n",
    "\n",
    "    # compute the output of both models\n",
    "    y_equivariant = eq_model.forward_features(x)\n",
    "    y_conventional = model.forward_features(x.tensor)\n",
    "    # y_conventional = eq_model.out_type(model.forward_features(x.tensor))\n",
    "\n",
    "    # create the mask for the output images\n",
    "    output_center_mask = torch.ones_like(y_equivariant)\n",
    "\n",
    "    # We evaluate the equivariance error on N=4 rotations\n",
    "\n",
    "    error_equivariant = []\n",
    "    error_conventional = []\n",
    "\n",
    "    # for each of the N rotations\n",
    "    for i in tqdm.tqdm(range(N)):\n",
    "        g = G.element((0, i))\n",
    "\n",
    "        # rotate the input\n",
    "        x_transformed = x.transform(g)\n",
    "        x_transformed.tensor *= input_center_mask\n",
    "\n",
    "        # F(g.X)  feed the transformed images in both models\n",
    "        y_from_x_transformed_equivariant = eq_model.forward_features(x_transformed)\n",
    "        y_from_x_transformed_conventional = model.forward_features(x_transformed.tensor)\n",
    "\n",
    "        # g.F(x)  transform the output of both models\n",
    "        y_transformed_from_x_equivariant = y_equivariant.transform(g)\n",
    "        # y_transformed_from_x_conventional = y_conventional.transform(g)\n",
    "        y_transformed_from_x_conventional = F.rotate(y_conventional, i / N * 2*np.pi)\n",
    "\n",
    "        # mask all the outputs\n",
    "        y_from_x_transformed_equivariant = y_from_x_transformed_equivariant * output_center_mask\n",
    "        y_from_x_transformed_conventional = y_from_x_transformed_conventional * output_center_mask\n",
    "        y_transformed_from_x_equivariant = y_transformed_from_x_equivariant.tensor * output_center_mask\n",
    "        # y_transformed_from_x_conventional = y_transformed_from_x_conventional.tensor * output_center_mask\n",
    "        y_transformed_from_x_conventional = y_transformed_from_x_conventional * output_center_mask\n",
    "\n",
    "        # compute the relative error of both models\n",
    "        rel_error_equivariant = torch.norm(y_from_x_transformed_equivariant - y_transformed_from_x_equivariant).item() / torch.norm(y_equivariant.tensor).item()\n",
    "        rel_error_conventional = torch.norm(y_from_x_transformed_conventional - y_transformed_from_x_conventional).item() / torch.norm(y_conventional).item()\n",
    "\n",
    "        error_equivariant.append(rel_error_equivariant)\n",
    "        error_conventional.append(rel_error_conventional)\n",
    "\n",
    "    # plot the error of both models as a function of the rotation angle theta\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    xs = [i / N * 2*np.pi for i in range(N)]\n",
    "    plt.plot(xs, error_equivariant, label='SO(2)-Steerable CNN')\n",
    "    plt.plot(xs, error_conventional, label='Conventional CNN')\n",
    "    plt.title('Equivariant vs Conventional CNNs', fontsize=20)\n",
    "    plt.xlabel(r'$g = r_\\theta$', fontsize=20)\n",
    "    plt.ylabel('Equivariance Error', fontsize=20)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=15)\n",
    "    plt.legend(fontsize=20)\n",
    "    i = 0\n",
    "    path = \"/home/adrian/pdp/equivariant-map-gen/equivariantgan/storage/figures/\"\n",
    "    name = \"compare_equivariance.png\"\n",
    "    while name in os.listdir(path):\n",
    "        i += 1\n",
    "        name = f\"compare_equivariance{i}.png\"\n",
    "    plt.savefig(path + name)\n",
    "\n",
    "\n",
    "def _test(config: DictConfig) -> None:\n",
    "    \"\"\"\n",
    "    Quick check, whether the model works\n",
    "    \"\"\"\n",
    "    datamodule = hydra.utils.instantiate(config.datamodule, _recursive_=False)\n",
    "    test_loader = datamodule.test_dataloader()\n",
    "    x = next(iter(test_loader))[\"input\"].squeeze()\n",
    "    lightning_model = hydra.utils.instantiate(config.lightning_model, _recursive_=False, _convert_=\"partial\")\n",
    "    eq_model = lightning_model.model.cpu()\n",
    "    eq_model.eval()\n",
    "    # model = timm.create_model(\"regnety_002\", pretrained=False)\n",
    "    kwargs = {\n",
    "        \"target\": \"classification.models.regnety.RegNetTimm\",\n",
    "        \"num_classes\": 10,\n",
    "        \"class_weights\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        \"model\": {\n",
    "            \"target\": \"timm.models.regnet.RegNet\",\n",
    "            \"cfg\": {\n",
    "                \"target\": \"classification.models.e2_regnety_simple.RegNetCfg\",\n",
    "                \"w0\": 16,\n",
    "                \"wa\": 7.5,\n",
    "                \"wm\": 1.83,\n",
    "                \"group_size\": 8,\n",
    "                \"depth\": 16,\n",
    "                \"se_ratio\": 0.25\n",
    "            },\n",
    "            \"in_chans\": 3,\n",
    "            \"output_stride\": 32,\n",
    "            \"global_pool\": \"avg\",\n",
    "            \"drop_rate\": 0.0,\n",
    "            \"drop_path_rate\": 0.0,\n",
    "            \"zero_init_last\": True,\n",
    "        }\n",
    "    }\n",
    "    lightning_config = dict(config.lightning_model)\n",
    "    lightning_config[\"model\"] = kwargs\n",
    "    lightning_config[\"lr_scheduler\"][\"t_initial\"] = 50\n",
    "    model = hydra.utils.instantiate(lightning_config, _recursive_=False, _convert_=\"partial\").model.cpu()\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        compare_two_models_relative_errors(x, model, eq_model)\n",
    "    \n",
    "\n",
    "if _name_ == \"_main_\":\n",
    "    _test()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
